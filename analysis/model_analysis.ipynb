{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from model_factory import ModelFactory\n",
    "sys.path.append('../utils')\n",
    "sys.path.append('../models')\n",
    "sys.path.append('../data_func')\n",
    "from data_helper_functions import create_study_periods,create_tensors\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "def evaluate_model_performance(model_state_path, train_test_splits, device, model_name, target_type, model_config):\n",
    "    factory = ModelFactory()\n",
    "    model, _ = factory.create(model_name, target_type, 'bce', model_config=model_config)  # Loss is not used in evaluation\n",
    "    model.load_state_dict(torch.load(model_state_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    accuracy_meter = AverageMeter()\n",
    "    # Additional metrics can be initialized here if needed\n",
    "    i=0\n",
    "    for split in tqdm(train_test_splits):\n",
    "        train_data, train_labels, test_data, test_labels = split\n",
    "\n",
    "        # Evaluating on training data\n",
    "        train_accuracy = compute_accuracy(model, train_data, train_labels, device)\n",
    "        accuracy_meter.update(train_accuracy, train_data.size(0))\n",
    "\n",
    "        # Evaluating on test data\n",
    "        test_accuracy = compute_accuracy(model, test_data, test_labels, device)\n",
    "        accuracy_meter.update(test_accuracy, test_data.size(0))\n",
    "\n",
    "        # Add additional metrics calculations here if needed\n",
    "        print(f'Accuracy for Period{i}: {test_accuracy}')\n",
    "        i+=1\n",
    "\n",
    "    return {\n",
    "        \"Average Accuracy\": accuracy_meter.avg,\n",
    "        # Add additional metrics here\n",
    "    }\n",
    "\n",
    "def compute_accuracy(model, data, labels, device):\n",
    "    dataset = TensorDataset(data, labels)\n",
    "    loader = DataLoader(dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs, _ = model(inputs)\n",
    "            predictions = torch.sigmoid(outputs).round()\n",
    "            \n",
    "            total_correct += (predictions.view(-1) == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "    accuracy = (total_correct / total_samples) * 100  # Convert to percentage\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count if self.count != 0 else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 34/38 [00:10<00:01,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached the end of the dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   1 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=6)]: Done   6 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed: 11.8min\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed: 15.9min\n",
      "[Parallel(n_jobs=6)]: Done  27 out of  34 | elapsed: 20.4min remaining:  5.3min\n",
      "[Parallel(n_jobs=6)]: Done  31 out of  34 | elapsed: 23.3min remaining:  2.3min\n",
      "[Parallel(n_jobs=6)]: Done  34 out of  34 | elapsed: 23.5min finished\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "target = 'cross_sectional_median'\n",
    "df = pd.read_csv('../data/crsp_ff_adjusted.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.dropna(subset=['RET'], inplace=True)\n",
    "df = df.drop(columns='Unnamed: 0')\n",
    "#subset df to 2014-2015\n",
    "# df = df[df['date'] >= datetime(2014, 1, 1)]\n",
    "\n",
    "# Create tensors\n",
    "study_periods = create_study_periods(df, n_periods=23, window_size=240, trade_size=250, train_size=750, forward_roll=250, \n",
    "                                        start_date=datetime(1990, 1, 1), end_date=datetime(2015, 12, 31), target_type=target)\n",
    "train_test_splits, task_types = create_tensors(study_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  5.1718],\n",
       "         [  5.6743],\n",
       "         [ -5.6945],\n",
       "         ...,\n",
       "         [ 15.8213],\n",
       "         [ -4.4009],\n",
       "         [ -0.3135]],\n",
       "\n",
       "        [[  5.6743],\n",
       "         [ -5.6945],\n",
       "         [  2.0816],\n",
       "         ...,\n",
       "         [ -4.4009],\n",
       "         [ -0.3135],\n",
       "         [ -5.4046]],\n",
       "\n",
       "        [[ -5.6945],\n",
       "         [  2.0816],\n",
       "         [-11.5747],\n",
       "         ...,\n",
       "         [ -0.3135],\n",
       "         [ -5.4046],\n",
       "         [  3.7353]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  0.1823],\n",
       "         [ -0.0800],\n",
       "         [  0.1459],\n",
       "         ...,\n",
       "         [  0.0899],\n",
       "         [ -0.2409],\n",
       "         [ -0.1160]],\n",
       "\n",
       "        [[ -0.0800],\n",
       "         [  0.1459],\n",
       "         [  2.5357],\n",
       "         ...,\n",
       "         [ -0.2409],\n",
       "         [ -0.1160],\n",
       "         [  0.4944]],\n",
       "\n",
       "        [[  0.1459],\n",
       "         [  2.5357],\n",
       "         [  0.7660],\n",
       "         ...,\n",
       "         [ -0.1160],\n",
       "         [  0.4944],\n",
       "         [  0.1136]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_splits[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>TICKER</th>\n",
       "      <th>RET</th>\n",
       "      <th>Adj_RET_Mkt</th>\n",
       "      <th>Adj_RET_Mkt_SMB</th>\n",
       "      <th>Adj_RET_Mkt_SMB_HML</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990-02-01</td>\n",
       "      <td>SUNW</td>\n",
       "      <td>0.012903</td>\n",
       "      <td>7.292903</td>\n",
       "      <td>8.532903</td>\n",
       "      <td>7.682903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990-02-01</td>\n",
       "      <td>MYG</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>7.294085</td>\n",
       "      <td>8.534085</td>\n",
       "      <td>7.684085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990-02-01</td>\n",
       "      <td>INTC</td>\n",
       "      <td>-0.012658</td>\n",
       "      <td>7.267342</td>\n",
       "      <td>8.507342</td>\n",
       "      <td>7.657342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990-02-01</td>\n",
       "      <td>CB</td>\n",
       "      <td>0.005634</td>\n",
       "      <td>7.285634</td>\n",
       "      <td>8.525634</td>\n",
       "      <td>7.675634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990-02-01</td>\n",
       "      <td>BUD</td>\n",
       "      <td>-0.026490</td>\n",
       "      <td>7.253510</td>\n",
       "      <td>8.493510</td>\n",
       "      <td>7.643510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266862</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>KMI</td>\n",
       "      <td>0.026135</td>\n",
       "      <td>-0.533865</td>\n",
       "      <td>-4.133865</td>\n",
       "      <td>-3.713865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266863</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>ADM</td>\n",
       "      <td>-0.005423</td>\n",
       "      <td>-0.565423</td>\n",
       "      <td>-4.165423</td>\n",
       "      <td>-3.745423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266864</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>HPE</td>\n",
       "      <td>-0.005236</td>\n",
       "      <td>-0.565236</td>\n",
       "      <td>-4.165236</td>\n",
       "      <td>-3.745236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266865</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>DIS</td>\n",
       "      <td>-0.011849</td>\n",
       "      <td>-0.571849</td>\n",
       "      <td>-4.171849</td>\n",
       "      <td>-3.751849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266866</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>-0.551936</td>\n",
       "      <td>-4.151936</td>\n",
       "      <td>-3.731936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3266867 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date TICKER       RET  Adj_RET_Mkt  Adj_RET_Mkt_SMB  \\\n",
       "0       1990-02-01   SUNW  0.012903     7.292903         8.532903   \n",
       "1       1990-02-01    MYG  0.014085     7.294085         8.534085   \n",
       "2       1990-02-01   INTC -0.012658     7.267342         8.507342   \n",
       "3       1990-02-01     CB  0.005634     7.285634         8.525634   \n",
       "4       1990-02-01    BUD -0.026490     7.253510         8.493510   \n",
       "...            ...    ...       ...          ...              ...   \n",
       "3266862 2015-12-31    KMI  0.026135    -0.533865        -4.133865   \n",
       "3266863 2015-12-31    ADM -0.005423    -0.565423        -4.165423   \n",
       "3266864 2015-12-31    HPE -0.005236    -0.565236        -4.165236   \n",
       "3266865 2015-12-31    DIS -0.011849    -0.571849        -4.171849   \n",
       "3266866 2015-12-31   TSLA  0.008064    -0.551936        -4.151936   \n",
       "\n",
       "         Adj_RET_Mkt_SMB_HML  \n",
       "0                   7.682903  \n",
       "1                   7.684085  \n",
       "2                   7.657342  \n",
       "3                   7.675634  \n",
       "4                   7.643510  \n",
       "...                      ...  \n",
       "3266862            -3.713865  \n",
       "3266863            -3.745423  \n",
       "3266864            -3.745236  \n",
       "3266865            -3.751849  \n",
       "3266866            -3.731936  \n",
       "\n",
       "[3266867 rows x 6 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/34 [23:19<12:49:48, 1399.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Period0: 94.99068944113658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/34 [41:20<22:44:17, 2480.52s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jacobbarcelona/Desktop/Thesis/analysis/model_analysis.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Thesis/analysis/model_analysis.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model_config \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Thesis/analysis/model_analysis.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39md_model\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m16\u001b[39m,  \u001b[39m# Update these parameters based on your model's configuration\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Thesis/analysis/model_analysis.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnum_heads\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m4\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Thesis/analysis/model_analysis.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdropout\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Thesis/analysis/model_analysis.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Thesis/analysis/model_analysis.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Ensure train_test_splits is defined and loaded as per your dataset\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Thesis/analysis/model_analysis.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# train_test_splits = [...]\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Thesis/analysis/model_analysis.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m performance_stats \u001b[39m=\u001b[39m evaluate_model_performance(model_state_path, train_test_splits, device, model_name, target_type, model_config)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Thesis/analysis/model_analysis.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(performance_stats)\n",
      "\u001b[1;32m/Users/jacobbarcelona/Desktop/Thesis/analysis/model_analysis.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Thesis/analysis/model_analysis.ipynb#W3sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m accuracy_meter\u001b[39m.\u001b[39mupdate(train_accuracy, train_data\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Thesis/analysis/model_analysis.ipynb#W3sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Evaluating on test data\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Thesis/analysis/model_analysis.ipynb#W3sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m test_accuracy \u001b[39m=\u001b[39m compute_accuracy(model, test_data, test_labels, device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Thesis/analysis/model_analysis.ipynb#W3sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m accuracy_meter\u001b[39m.\u001b[39mupdate(test_accuracy, test_data\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Thesis/analysis/model_analysis.ipynb#W3sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Add additional metrics calculations here if needed\u001b[39;00m\n",
      "\u001b[1;32m/Users/jacobbarcelona/Desktop/Thesis/analysis/model_analysis.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Thesis/analysis/model_analysis.ipynb#W3sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mfor\u001b[39;00m inputs, targets \u001b[39min\u001b[39;00m loader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Thesis/analysis/model_analysis.ipynb#W3sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     inputs, targets \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), targets\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Thesis/analysis/model_analysis.ipynb#W3sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     outputs, _ \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Thesis/analysis/model_analysis.ipynb#W3sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     predictions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(outputs)\u001b[39m.\u001b[39mround()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Thesis/analysis/model_analysis.ipynb#W3sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     total_correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (predictions\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m targets)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Thesis/analysis/../models/transformer_model.py:220\u001b[0m, in \u001b[0;36mTimeSeriesTransformer.forward\u001b[0;34m(self, src, src_mask)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39m# Pass through each layer of the encoder\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39mfor\u001b[39;00m layer_idx, layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_layers):\n\u001b[0;32m--> 220\u001b[0m     src \u001b[39m=\u001b[39m layer(src, src_mask)\n\u001b[1;32m    222\u001b[0m \u001b[39m# Capture the context from the last time step of the encoded sequence\u001b[39;00m\n\u001b[1;32m    223\u001b[0m context \u001b[39m=\u001b[39m src[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Thesis/analysis/../models/transformer_model.py:127\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, mask):\n\u001b[1;32m    124\u001b[0m \n\u001b[1;32m    125\u001b[0m \n\u001b[1;32m    126\u001b[0m     \u001b[39m# Self attention\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attention(x, x, x, mask)\n\u001b[1;32m    128\u001b[0m     \u001b[39m# Add and norm\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Thesis/analysis/../models/transformer_model.py:49\u001b[0m, in \u001b[0;36mScaledMultiHeadAttention.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m     46\u001b[0m V \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_v(value)\u001b[39m.\u001b[39mview(batch_size, seq_length, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[39m# Calculate attention scores with scaling for stability\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(Q, K\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)) \u001b[39m/\u001b[39;49m math\u001b[39m.\u001b[39;49msqrt(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead_dim)\n\u001b[1;32m     51\u001b[0m \u001b[39m# Generate relative positional embeddings and scores\u001b[39;00m\n\u001b[1;32m     52\u001b[0m relative_pos_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelative_positional_encoding(seq_length, batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "model_state_path = '../model_state_dict.pth'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Specify the model configuration\n",
    "model_name = 'transformer'  # Replace with your model's name if different\n",
    "target_type = 'classification'  # or 'regression', based on your model's task\n",
    "model_config = {\n",
    "    'd_model': 16,  # Update these parameters based on your model's configuration\n",
    "    'num_heads': 4,\n",
    "    'd_ff': 32,\n",
    "    'num_encoder_layers': 1,\n",
    "    'dropout': 0.1,\n",
    "}\n",
    "\n",
    "# Ensure train_test_splits is defined and loaded as per your dataset\n",
    "# train_test_splits = [...]\n",
    "\n",
    "performance_stats = evaluate_model_performance(model_state_path, train_test_splits, device, model_name, target_type, model_config)\n",
    "print(performance_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
