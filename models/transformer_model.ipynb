{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import math\n",
    "from datetime import datetime\n",
    "#import relu\n",
    "from torch.nn import functional as F\n",
    "sys.path.append('../models')\n",
    "sys.path.append('../data_func')\n",
    "from data_helper_functions import create_study_periods,create_tensors\n",
    "from transformer_model import TimeSeriesTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>TICKER</th>\n",
       "      <th>RET</th>\n",
       "      <th>Adj_RET_Mkt</th>\n",
       "      <th>Adj_RET_Mkt_SMB</th>\n",
       "      <th>Adj_RET_Mkt_SMB_HML</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990-02-01</td>\n",
       "      <td>SUNW</td>\n",
       "      <td>0.012903</td>\n",
       "      <td>7.292903</td>\n",
       "      <td>8.532903</td>\n",
       "      <td>7.682903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990-02-01</td>\n",
       "      <td>MYG</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>7.294085</td>\n",
       "      <td>8.534085</td>\n",
       "      <td>7.684085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990-02-01</td>\n",
       "      <td>INTC</td>\n",
       "      <td>-0.012658</td>\n",
       "      <td>7.267342</td>\n",
       "      <td>8.507342</td>\n",
       "      <td>7.657342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990-02-01</td>\n",
       "      <td>CB</td>\n",
       "      <td>0.005634</td>\n",
       "      <td>7.285634</td>\n",
       "      <td>8.525634</td>\n",
       "      <td>7.675634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990-02-01</td>\n",
       "      <td>BUD</td>\n",
       "      <td>-0.026490</td>\n",
       "      <td>7.253510</td>\n",
       "      <td>8.493510</td>\n",
       "      <td>7.643510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266862</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>KMI</td>\n",
       "      <td>0.026135</td>\n",
       "      <td>-0.533865</td>\n",
       "      <td>-4.133865</td>\n",
       "      <td>-3.713865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266863</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>ADM</td>\n",
       "      <td>-0.005423</td>\n",
       "      <td>-0.565423</td>\n",
       "      <td>-4.165423</td>\n",
       "      <td>-3.745423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266864</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>HPE</td>\n",
       "      <td>-0.005236</td>\n",
       "      <td>-0.565236</td>\n",
       "      <td>-4.165236</td>\n",
       "      <td>-3.745236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266865</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>DIS</td>\n",
       "      <td>-0.011849</td>\n",
       "      <td>-0.571849</td>\n",
       "      <td>-4.171849</td>\n",
       "      <td>-3.751849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266866</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>-0.551936</td>\n",
       "      <td>-4.151936</td>\n",
       "      <td>-3.731936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3266867 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date TICKER       RET  Adj_RET_Mkt  Adj_RET_Mkt_SMB  \\\n",
       "0       1990-02-01   SUNW  0.012903     7.292903         8.532903   \n",
       "1       1990-02-01    MYG  0.014085     7.294085         8.534085   \n",
       "2       1990-02-01   INTC -0.012658     7.267342         8.507342   \n",
       "3       1990-02-01     CB  0.005634     7.285634         8.525634   \n",
       "4       1990-02-01    BUD -0.026490     7.253510         8.493510   \n",
       "...            ...    ...       ...          ...              ...   \n",
       "3266862 2015-12-31    KMI  0.026135    -0.533865        -4.133865   \n",
       "3266863 2015-12-31    ADM -0.005423    -0.565423        -4.165423   \n",
       "3266864 2015-12-31    HPE -0.005236    -0.565236        -4.165236   \n",
       "3266865 2015-12-31    DIS -0.011849    -0.571849        -4.171849   \n",
       "3266866 2015-12-31   TSLA  0.008064    -0.551936        -4.151936   \n",
       "\n",
       "         Adj_RET_Mkt_SMB_HML  \n",
       "0                   7.682903  \n",
       "1                   7.684085  \n",
       "2                   7.657342  \n",
       "3                   7.675634  \n",
       "4                   7.643510  \n",
       "...                      ...  \n",
       "3266862            -3.713865  \n",
       "3266863            -3.745423  \n",
       "3266864            -3.745236  \n",
       "3266865            -3.751849  \n",
       "3266866            -3.731936  \n",
       "\n",
       "[3266867 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('../data/crsp_ff_adjusted.csv')\n",
    "#drop unamed 0\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.dropna(subset=['RET'],inplace=True)\n",
    "df=df.drop(columns='Unnamed: 0')\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #select returns to use\n",
    "# returns='RET'\n",
    "# df=df[['date','TICKER',f'{returns}']]\n",
    "# if returns!='RET':\n",
    "#     #rename returns column\n",
    "#     df.rename(columns={f'{returns}':'RET'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 34/38 [00:06<00:00,  5.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached the end of the dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Optional parameter target_type: 'cross_sectional_median(default)','buckets(10 buckets)','raw_returns'.\n",
    "study_periods=create_study_periods(df,n_periods=23,window_size=240,trade_size=250,train_size=750,forward_roll=250,start_date=datetime(1990,1,1),end_date=datetime(2015,12,31),target_type='cross_sectional_median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   1 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=6)]: Done   6 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=6)]: Done  27 out of  34 | elapsed: 16.1min remaining:  4.2min\n",
      "[Parallel(n_jobs=6)]: Done  31 out of  34 | elapsed: 18.1min remaining:  1.8min\n",
      "[Parallel(n_jobs=6)]: Done  34 out of  34 | elapsed: 18.5min finished\n"
     ]
    }
   ],
   "source": [
    "train_test_splits,task_types=create_tensors(study_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([248383])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Optional code to verify tensor shapes\n",
    "# for train_data, train_labels, test_data, test_labels in train_test_splits:\n",
    "#     print(train_data.shape, train_labels.shape, test_data.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/34 [48:40<26:46:26, 2920.80s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jacobbarcelona/Desktop/Deep Learning Statistical Arbitrage/models/transformer_model.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m data, labels \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(data, src_mask\u001b[39m=\u001b[39;49mtrain_mask)  \u001b[39m# Adjusted here to use the look-ahead mask\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs\u001b[39m.\u001b[39msqueeze(), labels\u001b[39m.\u001b[39mfloat())  \u001b[39m# Adjust based on your specific use case\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/jacobbarcelona/Desktop/Deep Learning Statistical Arbitrage/models/transformer_model.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39m# Pass through each layer of the encoder\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_layers:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m     src \u001b[39m=\u001b[39m layer(src, src_mask)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m \u001b[39m# Capture the context from the last time step of the encoded sequence\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m context \u001b[39m=\u001b[39m src[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/jacobbarcelona/Desktop/Deep Learning Statistical Arbitrage/models/transformer_model.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x,mask):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39m#Self attention\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     attention\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attention(x,x,x,mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39m#Add and norm\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm1(x\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/jacobbarcelona/Desktop/Deep Learning Statistical Arbitrage/models/transformer_model.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mif\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# Look ahead mask to prevent information leakage\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     key_out \u001b[39m=\u001b[39m key_out\u001b[39m.\u001b[39mmasked_fill(mask \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-inf\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m attention \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49msoftmax(key_out, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)  \u001b[39m# Apply softmax to get probabilities\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m value_out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(attention, V)  \u001b[39m# Multiply attention scores by value\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m value_out \u001b[39m=\u001b[39m value_out\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dim)  \u001b[39m# Reshape to get back to original shape\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Check if CUDA, MPS, or CPU should be used\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "best_model_path = \"best_model.pth\" \n",
    "model = TimeSeriesTransformer(d_model=64, num_heads=8, d_ff=256, num_encoder_layers=2, \n",
    "                               dropout=.1, max_len=240,task_type='classification').to(device)\n",
    "\n",
    "# Loss depends on target, MAE for returns, Cross Entropy for above/below cross-sectional median. Also have selective loss in utils\n",
    "if task_types[0] == 'classification':\n",
    "        criterion = nn.NLLLoss()\n",
    "else:\n",
    "    criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs = 1\n",
    "patience = 5\n",
    "best_loss = np.inf\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    for train_data, train_labels, val_data, val_labels in tqdm(train_test_splits):\n",
    "\n",
    "        # Generate look-ahead masks\n",
    "        train_mask = ScaledMultiHeadAttention.create_look_ahead_mask(train_data.size(1)).to(device)\n",
    "        val_mask = ScaledMultiHeadAttention.create_look_ahead_mask(val_data.size(1)).to(device)\n",
    "\n",
    "\n",
    "        train_dataset = TensorDataset(train_data, train_labels)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "        val_dataset = TensorDataset(val_data, val_labels)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "        train_loss = 0.0\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data, src_mask=train_mask).squeeze()\n",
    "            if task_types[0] == 'classification':\n",
    "                labels = labels.long()  # Adjusted here to use the look-ahead mask\n",
    "            loss = criterion(outputs, labels)  # Adjust based on your specific use case\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        total_train_loss += train_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(data, src_mask=train_mask).squeeze() \n",
    "                if task_types[0] == 'classification':\n",
    "                    labels = labels.long() # Adjusted here to use the look-ahead mask\n",
    "                loss = criterion(outputs.squeeze(), labels)  # Adjust based on your specific use case\n",
    "                val_loss += loss.item() * data.size(0)\n",
    "\n",
    "        total_val_loss += val_loss / len(val_loader.dataset)\n",
    "\n",
    "    average_train_loss = total_train_loss / len(train_test_splits)\n",
    "    average_val_loss = total_val_loss / len(train_test_splits)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{n_epochs}, '\n",
    "          f'Average Train Loss: {average_train_loss:.4f}, '\n",
    "          f'Average Validation Loss: {average_val_loss:.4f}')\n",
    "\n",
    "    if average_val_loss < best_loss:\n",
    "        best_loss = average_val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter == patience:\n",
    "        print('Early stopping!')\n",
    "        break\n",
    "\n",
    "best_model_state = torch.load(best_model_path, map_location=device)\n",
    "model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sharpe ratio objective function?\n",
    "# for epoch in range(n_epochs):\n",
    "#     model.train()\n",
    "#     total_train_loss = 0.0\n",
    "#     total_val_loss = 0.0\n",
    "\n",
    "#     for train_data, train_labels, val_data, val_labels in train_test_splits:\n",
    "#         # Make sure your labels are in the correct shape\n",
    "#         train_labels = train_labels.view(-1, 1).to(device)\n",
    "#         val_labels = val_labels.view(-1, 1).to(device)\n",
    "\n",
    "#         # Training section\n",
    "#         train_mask = ScaledMultiHeadAttention.create_look_ahead_mask(train_data.size(1)).to(device)\n",
    "#         train_data = train_data.to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         weights = model(train_data, src_mask=train_mask)\n",
    "#         abs_sum = torch.sum(torch.abs(weights), axis=1, keepdim=True) + 1e-8  # Avoid division by zero\n",
    "#         weights = weights / abs_sum\n",
    "\n",
    "#         rets = torch.sum(weights * train_labels, axis=1)  \n",
    "\n",
    "#         mean_ret = torch.mean(rets)\n",
    "#         std = torch.std(rets) + 1e-8  # Avoid division by zero\n",
    "#         sharpe_ratio = mean_ret / std  \n",
    "\n",
    "#         train_loss = -sharpe_ratio\n",
    "#         train_loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_train_loss += train_loss.item()\n",
    "\n",
    "#         # Validation section\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             val_mask = ScaledMultiHeadAttention.create_look_ahead_mask(val_data.size(1)).to(device)\n",
    "#             val_data = val_data.to(device)\n",
    "\n",
    "#             val_weights = model(val_data, src_mask=val_mask)\n",
    "#             val_abs_sum = torch.sum(torch.abs(val_weights), axis=1, keepdim=True) + 1e-8  # Avoid division by zero\n",
    "#             val_weights = val_weights / val_abs_sum\n",
    "            \n",
    "#             val_rets = torch.sum(val_weights * val_labels, axis=1)  \n",
    "\n",
    "#             val_mean_ret = torch.mean(val_rets)\n",
    "#             val_std = torch.std(val_rets) + 1e-8  # Avoid division by zero\n",
    "#             val_sharpe_ratio = val_mean_ret / val_std\n",
    "\n",
    "#             val_loss = -val_sharpe_ratio\n",
    "#             total_val_loss += val_loss.item()\n",
    "\n",
    "#     avg_train_loss = total_train_loss / len(train_test_splits)\n",
    "#     avg_val_loss = total_val_loss / len(train_test_splits)\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}/{n_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "#     # Inside your training loop\n",
    "#     if avg_val_loss < best_loss:\n",
    "#         best_loss = avg_val_loss\n",
    "#         torch.save(model.state_dict(), 'best_model.pth')\n",
    "#         counter = 0\n",
    "#     else:\n",
    "#         counter += 1\n",
    "\n",
    "#     if counter == patience:\n",
    "#         print('Early stopping!')\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TimeSeriesTransformer(d_model=64, num_heads=8, d_ff=256, num_encoder_layers=2, \n",
    "                               dropout=.1, max_len=240,task_type='classification')\n",
    "model.load_state_dict(torch.load('best_model.pth',map_location=torch.device('cpu')) )\n",
    "model.eval()\n",
    "\n",
    "in_sample_long_portfolios = pd.DataFrame()\n",
    "out_of_sample_long_portfolios = pd.DataFrame()\n",
    "\n",
    "in_sample_short_portfolios = pd.DataFrame()\n",
    "out_of_sample_short_portfolios = pd.DataFrame()\n",
    "\n",
    "k = 10  # Number of top assets to select in portfolios\n",
    "\n",
    "for train_data, train_labels, val_data, val_labels in tqdm(train_test_splits):\n",
    "    # Here, train_data, val_data are your training and validation data respectively\n",
    "    \n",
    "    train_mask = ScaledMultiHeadAttention.create_look_ahead_mask(train_data.size(1))\n",
    "    val_mask = ScaledMultiHeadAttention.create_look_ahead_mask(val_data.size(1))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_predictions = model(train_data.to(device), src_mask=train_mask.to(device))\n",
    "        val_predictions = model(val_data.to(device), src_mask=val_mask.to(device))\n",
    "\n",
    "        train_probs = torch.softmax(train_predictions, dim=1)[:, 1].cpu().numpy()\n",
    "        val_probs = torch.softmax(val_predictions, dim=1)[:, 1].cpu().numpy()\n",
    "\n",
    "    # Assuming you have a dataframe or similar structure to hold the date and TICKER information\n",
    "    train_df['predicted_prob'] = train_probs\n",
    "    val_df['predicted_prob'] = val_probs\n",
    "\n",
    "    # In-Sample Portfolio Construction\n",
    "    for date in train_df['date'].unique():\n",
    "        date_data = train_df[train_df['date'] == date].sort_values(by='predicted_prob', ascending=False)\n",
    "        \n",
    "        long_tickers = date_data.head(k)\n",
    "        short_tickers = date_data.tail(k)\n",
    "        \n",
    "        in_sample_long_portfolios = pd.concat([in_sample_long_portfolios, long_tickers])\n",
    "        in_sample_short_portfolios = pd.concat([in_sample_short_portfolios, short_tickers])\n",
    "\n",
    "    # Out-of-Sample Portfolio Construction\n",
    "    for date in val_df['date'].unique():\n",
    "        date_data = val_df[val_df['date'] == date].sort_values(by='predicted_prob', ascending=False)\n",
    "        \n",
    "        long_tickers = date_data.head(k)\n",
    "        short_tickers = date_data.tail(k)\n",
    "        \n",
    "        out_of_sample_long_portfolios = pd.concat([out_of_sample_long_portfolios, long_tickers])\n",
    "        out_of_sample_short_portfolios = pd.concat([out_of_sample_short_portfolios, short_tickers])\n",
    "\n",
    "# At this point, in_sample_long_portfolios, out_of_sample_long_portfolios, etc. hold your portfolios\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sample_long_portfolios.to_csv('../data/transformer_results/in_sample_long_portfolios.csv')\n",
    "in_sample_short_portfolios.to_csv('../data/transformer_results/in_sample_short_portfolios.csv')\n",
    "out_of_sample_long_portfolios.to_csv('../data/transformer_results/out_of_sample_long_portfolios.csv')\n",
    "out_of_sample_short_portfolios.to_csv('../data/transformer_results/out_of_sample_short_portfolios.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
