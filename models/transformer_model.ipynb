{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import math\n",
    "from datetime import datetime\n",
    "#import relu\n",
    "from torch.nn import functional as F\n",
    "sys.path.append('../data')\n",
    "from data_helper_functions import create_study_periods,create_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>TICKER</th>\n",
       "      <th>RET</th>\n",
       "      <th>Adj_RET_Mkt</th>\n",
       "      <th>Adj_RET_Mkt_SMB</th>\n",
       "      <th>Adj_RET_Mkt_SMB_HML</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990-02-01</td>\n",
       "      <td>SUNW</td>\n",
       "      <td>0.012903</td>\n",
       "      <td>7.292903</td>\n",
       "      <td>8.532903</td>\n",
       "      <td>7.682903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990-02-01</td>\n",
       "      <td>MYG</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>7.294085</td>\n",
       "      <td>8.534085</td>\n",
       "      <td>7.684085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990-02-01</td>\n",
       "      <td>INTC</td>\n",
       "      <td>-0.012658</td>\n",
       "      <td>7.267342</td>\n",
       "      <td>8.507342</td>\n",
       "      <td>7.657342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990-02-01</td>\n",
       "      <td>CB</td>\n",
       "      <td>0.005634</td>\n",
       "      <td>7.285634</td>\n",
       "      <td>8.525634</td>\n",
       "      <td>7.675634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990-02-01</td>\n",
       "      <td>BUD</td>\n",
       "      <td>-0.026490</td>\n",
       "      <td>7.253510</td>\n",
       "      <td>8.493510</td>\n",
       "      <td>7.643510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266862</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>KMI</td>\n",
       "      <td>0.026135</td>\n",
       "      <td>-0.533865</td>\n",
       "      <td>-4.133865</td>\n",
       "      <td>-3.713865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266863</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>ADM</td>\n",
       "      <td>-0.005423</td>\n",
       "      <td>-0.565423</td>\n",
       "      <td>-4.165423</td>\n",
       "      <td>-3.745423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266864</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>HPE</td>\n",
       "      <td>-0.005236</td>\n",
       "      <td>-0.565236</td>\n",
       "      <td>-4.165236</td>\n",
       "      <td>-3.745236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266865</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>DIS</td>\n",
       "      <td>-0.011849</td>\n",
       "      <td>-0.571849</td>\n",
       "      <td>-4.171849</td>\n",
       "      <td>-3.751849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266866</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>-0.551936</td>\n",
       "      <td>-4.151936</td>\n",
       "      <td>-3.731936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3266867 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date TICKER       RET  Adj_RET_Mkt  Adj_RET_Mkt_SMB  \\\n",
       "0       1990-02-01   SUNW  0.012903     7.292903         8.532903   \n",
       "1       1990-02-01    MYG  0.014085     7.294085         8.534085   \n",
       "2       1990-02-01   INTC -0.012658     7.267342         8.507342   \n",
       "3       1990-02-01     CB  0.005634     7.285634         8.525634   \n",
       "4       1990-02-01    BUD -0.026490     7.253510         8.493510   \n",
       "...            ...    ...       ...          ...              ...   \n",
       "3266862 2015-12-31    KMI  0.026135    -0.533865        -4.133865   \n",
       "3266863 2015-12-31    ADM -0.005423    -0.565423        -4.165423   \n",
       "3266864 2015-12-31    HPE -0.005236    -0.565236        -4.165236   \n",
       "3266865 2015-12-31    DIS -0.011849    -0.571849        -4.171849   \n",
       "3266866 2015-12-31   TSLA  0.008064    -0.551936        -4.151936   \n",
       "\n",
       "         Adj_RET_Mkt_SMB_HML  \n",
       "0                   7.682903  \n",
       "1                   7.684085  \n",
       "2                   7.657342  \n",
       "3                   7.675634  \n",
       "4                   7.643510  \n",
       "...                      ...  \n",
       "3266862            -3.713865  \n",
       "3266863            -3.745423  \n",
       "3266864            -3.745236  \n",
       "3266865            -3.751849  \n",
       "3266866            -3.731936  \n",
       "\n",
       "[3266867 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('../data/crsp_ff_adjusted.csv')\n",
    "#drop unamed 0\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.dropna(subset=['RET'],inplace=True)\n",
    "df=df.drop(columns='Unnamed: 0')\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 34/38 [00:06<00:00,  5.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached the end of the dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "study_periods=create_study_periods(df,n_periods=23,window_size=240,trade_size=250,train_size=750,forward_roll=250,start_date=datetime(1990,1,1),end_date=datetime(2015,12,31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   1 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=6)]: Done   6 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed: 26.5min\n",
      "[Parallel(n_jobs=6)]: Done  27 out of  34 | elapsed: 98.3min remaining: 25.5min\n",
      "[Parallel(n_jobs=6)]: Done  31 out of  34 | elapsed: 100.1min remaining:  9.7min\n",
      "[Parallel(n_jobs=6)]: Done  34 out of  34 | elapsed: 100.1min finished\n"
     ]
    }
   ],
   "source": [
    "train_test_splits=create_tensors(study_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([248383])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_splits[0][1].shape   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, n_dim):\n",
    "        super(ScaledMultiHeadAttention, self).__init__()\n",
    "        assert n_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.n_dim = n_dim\n",
    "        self.head_dim = n_dim // num_heads  # Dimension of each head: commonly referred to as d_k\n",
    "\n",
    "        self.fc_q = nn.Linear(n_dim, n_dim)  # Query\n",
    "        self.fc_k = nn.Linear(n_dim, n_dim)  # Key\n",
    "        self.fc_v = nn.Linear(n_dim, n_dim)  # Value\n",
    "        self.fc_o = nn.Linear(n_dim, n_dim)  # Output\n",
    "\n",
    "    def create_look_ahead_mask(size):\n",
    "        mask = 1 - torch.tril(torch.ones((size, size)))\n",
    "        return mask  # Returns 0 for positions to be masked\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "        Q = self.fc_q(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.fc_k(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.fc_v(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        key_out = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)  # Dot product of query and key to get attention scores\n",
    "\n",
    "        if mask is not None:  # Look ahead mask to prevent information leakage\n",
    "            key_out = key_out.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attention = torch.softmax(key_out, dim=-1)  # Apply softmax to get probabilities\n",
    "        value_out = torch.matmul(attention, V)  # Multiply attention scores by value\n",
    "        value_out = value_out.transpose(1, 2).contiguous().view(batch_size, -1, self.n_dim)  # Reshape to get back to original shape\n",
    "        return self.fc_o(value_out)  # Apply final linear layer\n",
    "\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self,d_model,d_ff,dropout=.1):\n",
    "        super(PositionWiseFeedForward,self).__init__()\n",
    "        self.linear1=nn.Linear(d_model,d_ff)\n",
    "        self.linear2=nn.Linear(d_ff,d_model)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    def forward(self,x):\n",
    "        #RELU activation\n",
    "        x=self.linear1(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.dropout(x)\n",
    "        x=self.linear2(x)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,num_heads,d_ff,dropout=.1):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.self_attention=ScaledMultiHeadAttention(num_heads,d_model)\n",
    "        self.position_wise_feed_forward=PositionWiseFeedForward(d_model,d_ff,dropout)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.layer_norm1=nn.LayerNorm(d_model)\n",
    "        self.layer_norm2=nn.LayerNorm(d_model)\n",
    "    def forward(self,x,mask):\n",
    "        #Self attention\n",
    "        attention=self.self_attention(x,x,x,mask)\n",
    "        #Add and norm\n",
    "        x=self.layer_norm1(x+self.dropout(attention))\n",
    "        #Position wise feed forward\n",
    "        feed_forward=self.position_wise_feed_forward(x)\n",
    "        #Add and norm\n",
    "        x=self.layer_norm2(x+self.dropout(feed_forward))\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_encoder_layers, dropout=0.1, max_len=512):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "\n",
    "        # Final linear layer to denoise output\n",
    "        self.fc = nn.Linear(d_model, 1)  \n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        # Add positional encoding to the input\n",
    "        src = self.positional_encoding(src)\n",
    "        \n",
    "        # Pass through each layer of the encoder\n",
    "        for layer in self.encoder_layers:\n",
    "            src = layer(src, src_mask)\n",
    "        \n",
    "        # Capture the context from the last time step of the encoded sequence\n",
    "        context = src[:, -1, :]\n",
    "        \n",
    "        # Final linear layer\n",
    "        output = self.fc(context)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/34 [48:40<26:46:26, 2920.80s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jacobbarcelona/Desktop/Deep Learning Statistical Arbitrage/models/transformer_model.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m data, labels \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(data, src_mask\u001b[39m=\u001b[39;49mtrain_mask)  \u001b[39m# Adjusted here to use the look-ahead mask\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs\u001b[39m.\u001b[39msqueeze(), labels\u001b[39m.\u001b[39mfloat())  \u001b[39m# Adjust based on your specific use case\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/jacobbarcelona/Desktop/Deep Learning Statistical Arbitrage/models/transformer_model.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39m# Pass through each layer of the encoder\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_layers:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m     src \u001b[39m=\u001b[39m layer(src, src_mask)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m \u001b[39m# Capture the context from the last time step of the encoded sequence\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m context \u001b[39m=\u001b[39m src[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/jacobbarcelona/Desktop/Deep Learning Statistical Arbitrage/models/transformer_model.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x,mask):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39m#Self attention\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     attention\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attention(x,x,x,mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39m#Add and norm\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm1(x\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/jacobbarcelona/Desktop/Deep Learning Statistical Arbitrage/models/transformer_model.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mif\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# Look ahead mask to prevent information leakage\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     key_out \u001b[39m=\u001b[39m key_out\u001b[39m.\u001b[39mmasked_fill(mask \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-inf\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m attention \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49msoftmax(key_out, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)  \u001b[39m# Apply softmax to get probabilities\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m value_out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(attention, V)  \u001b[39m# Multiply attention scores by value\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacobbarcelona/Desktop/Deep%20Learning%20Statistical%20Arbitrage/models/transformer_model.ipynb#X12sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m value_out \u001b[39m=\u001b[39m value_out\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dim)  \u001b[39m# Reshape to get back to original shape\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = TimeSeriesTransformer(d_model=64, num_heads=8, d_ff=256, num_encoder_layers=2, \n",
    "                               dropout=.1, max_len=240).to(device)\n",
    "\n",
    "# Loss depends on target, MAE for returns, Cross Entropy for above/below cross-sectional median\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs = 100\n",
    "patience = 5\n",
    "best_loss = np.inf\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    for train_data, train_labels, val_data, val_labels in tqdm(train_test_splits):\n",
    "\n",
    "        # Generate look-ahead masks\n",
    "        train_mask = ScaledMultiHeadAttention.create_look_ahead_mask(train_data.size(1))\n",
    "        val_mask = ScaledMultiHeadAttention.create_look_ahead_mask(val_data.size(1))\n",
    "\n",
    "        train_dataset = TensorDataset(train_data, train_labels)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "        val_dataset = TensorDataset(val_data, val_labels)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "        train_loss = 0.0\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data, src_mask=train_mask)  # Adjusted here to use the look-ahead mask\n",
    "            loss = criterion(outputs.squeeze(), labels.float())  # Adjust based on your specific use case\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        total_train_loss += train_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(data, src_mask=val_mask)  # Adjusted here to use the look-ahead mask\n",
    "                loss = criterion(outputs.squeeze(), labels.float())  # Adjust based on your specific use case\n",
    "                val_loss += loss.item() * data.size(0)\n",
    "\n",
    "        total_val_loss += val_loss / len(val_loader.dataset)\n",
    "\n",
    "    average_train_loss = total_train_loss / len(train_test_splits)\n",
    "    average_val_loss = total_val_loss / len(train_test_splits)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{n_epochs}, '\n",
    "          f'Average Train Loss: {average_train_loss:.4f}, '\n",
    "          f'Average Validation Loss: {average_val_loss:.4f}')\n",
    "\n",
    "    if average_val_loss < best_loss:\n",
    "        best_loss = average_val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter == patience:\n",
    "        print('Early stopping!')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model_state)\n",
    "model.eval()\n",
    "predictions = []\n",
    "returns_df = df[['date', 'TICKER', 'RET']].copy()\n",
    "# Concatenate training and testing data and labels\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "# This will store DataLoaders for the entire dataset for each period\n",
    "entire_loaders = []\n",
    "\n",
    "for train_loader, test_loader in zip(train_loaders, test_loaders):\n",
    "    # We access the .dataset attribute of the DataLoader to get the underlying dataset\n",
    "    entire_dataset = ConcatDataset([train_loader.dataset, test_loader.dataset])\n",
    "    \n",
    "    # Creating a DataLoader for the entire concatenated dataset\n",
    "    entire_loader = DataLoader(entire_dataset, batch_size=64, shuffle=False)  # We set shuffle to False as we're not training the model here\n",
    "    \n",
    "    entire_loaders.append(entire_loader)\n",
    "\n",
    "# Now, entire_loaders contains DataLoaders for the entire dataset (train + test) for each period\n",
    "\n",
    "# You can now proceed to make predictions for each period like this:\n",
    "all_predictions = []  # This will store predictions for all periods\n",
    "\n",
    "for entire_loader in tqdm(entire_loaders):\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in entire_loader:  \n",
    "            sequences = sequences.unsqueeze(-1)\n",
    "            outputs = model(sequences)\n",
    "            probabilities = torch.softmax(outputs, dim=1)[:, 1].numpy() \n",
    "            predictions.extend(probabilities)\n",
    "            \n",
    "    all_predictions.append(np.array(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
