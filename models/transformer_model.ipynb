{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import math\n",
    "from datetime import datetime\n",
    "#import relu\n",
    "from torch.nn import functional as F\n",
    "sys.path.append('../models')\n",
    "sys.path.append('../data_func')\n",
    "from data_helper_functions import create_study_periods,create_tensors\n",
    "from transformer_model import TimeSeriesTransformer,ScaledMultiHeadAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train multiple transformer models. 1. Cross-sectional-Median Target 2. Raw-returns target 3. Sharpe ratio objective 4. Selective Transformer Model with all of the previous iterations. We can also consider Stochastic Attention, using Attention to calculate a confidence score, and building a confidence model as a first pass. (Cross-sectional median confidence as first filter, and then use the confidence score to select the top 10% of stocks to train on.)\n",
    "All in all, because of compute the goal shouldn't be to train a model that generates an insane sharpe, but show the potential of Selective ML in portfolio building and how it improves over current SOTA methods. The paper should mostly be an analysis on what types of stocks the model does not learn from/ abstains from.  We can't afford to hyper-parameter tune a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv('../data/crsp_ff_adjusted.csv')\n",
    "# #drop unamed 0\n",
    "# df['date'] = pd.to_datetime(df['date'])\n",
    "# df.dropna(subset=['RET'],inplace=True)\n",
    "# df=df.drop(columns='Unnamed: 0')\n",
    "# df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get just 2014\n",
    "# df=df[df['date'].dt.year==2014]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "np.random.seed(42)  # for reproducibility\n",
    "\n",
    "def generate_individual_stock_data(num_days=1000, mu=0.1, sigma=0.5, start_price=100):\n",
    "    price_changes = np.random.randn(num_days) * sigma + mu  # daily returns\n",
    "    prices = np.cumsum(price_changes) + start_price  # price series\n",
    "    return prices\n",
    "\n",
    "def process_stock_data(prices, sequence_length=250):\n",
    "    # Calculate log returns from prices\n",
    "    log_returns = np.log(prices[1:] / prices[:-1])\n",
    "    \n",
    "    # Generate sequences and labels\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i in range(len(log_returns) - sequence_length):\n",
    "        data.append(log_returns[i:i + sequence_length])\n",
    "        labels.append(log_returns[i + sequence_length])\n",
    "    \n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Set parameters\n",
    "num_days = 1000\n",
    "sequence_length = 250\n",
    "train_period = 750\n",
    "\n",
    "# Generate data for each stock\n",
    "stocks_data = []\n",
    "for i in range(3):  # For 3 stocks\n",
    "    prices = generate_individual_stock_data(num_days=num_days, mu=(i+1)*0.1, sigma=0.5, start_price=(i+1)*100)\n",
    "    stock_data, stock_labels = process_stock_data(prices, sequence_length=sequence_length)\n",
    "    stocks_data.append((stock_data[:train_period], stock_labels[:train_period],  # Training data\n",
    "                        stock_data[train_period-sequence_length:],  # Test data (with overlap)\n",
    "                        stock_labels[train_period-sequence_length:]))\n",
    "\n",
    "# Prepare the final datasets\n",
    "train_data_combined = np.concatenate([data[0] for data in stocks_data], axis=0)\n",
    "train_labels_combined = np.concatenate([data[1] for data in stocks_data], axis=0)\n",
    "test_data_combined = np.concatenate([data[2] for data in stocks_data], axis=0)\n",
    "test_labels_combined = np.concatenate([data[3] for data in stocks_data], axis=0)\n",
    "\n",
    "# Convert to tensors\n",
    "train_data_tensor = torch.tensor(train_data_combined, dtype=torch.float32).unsqueeze(-1)  # Add an extra dimension\n",
    "train_labels_tensor = torch.tensor(train_labels_combined, dtype=torch.float32)\n",
    "test_data_tensor = torch.tensor(test_data_combined, dtype=torch.float32).unsqueeze(-1)\n",
    "test_labels_tensor = torch.tensor(test_labels_combined, dtype=torch.float32)\n",
    "\n",
    "train_test_splits=[(train_data_tensor,train_labels_tensor,test_data_tensor,test_labels_tensor)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #select returns to use\n",
    "# returns='RET'\n",
    "# df=df[['date','TICKER',f'{returns}']]\n",
    "# if returns!='RET':\n",
    "#     #rename returns column\n",
    "#     df.rename(columns={f'{returns}':'RET'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional parameter target_type: 'cross_sectional_median(default)','buckets(10 buckets)','raw_returns'.\n",
    "# study_periods=create_study_periods(df,n_periods=23,window_size=240,trade_size=250,train_size=750,forward_roll=250,start_date=datetime(1990,1,1),end_date=datetime(2015,12,31),target_type='raw_return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_splits,task_types=create_tensors(study_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional code to verify tensor shapes\n",
    "# for train_data, train_labels, test_data, test_labels in train_test_splits:\n",
    "#     print(train_data.shape, train_labels.shape, test_data.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_types=['regression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOOK INTO MAKING THE D_MODEL>1 YOU'LL HAVE TO STACK INPUTS AS CURRENT INPUT IS [BATCH,SEQUENCE,1] NOT [BATCH,SEQUENCE,D_MODEL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Check if CUDA, MPS, or CPU should be used\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "best_model_path = \"best_model.pth\" \n",
    "model = TimeSeriesTransformer(d_model=1, num_heads=1, d_ff=256, num_encoder_layers=2, \n",
    "                               dropout=.1,task_type=task_types[0]).to(device)\n",
    "\n",
    "# Loss depends on target, MAE for returns, Cross Entropy for above/below cross-sectional median. Also have selective loss in utils\n",
    "if task_types[0] == 'classification':\n",
    "    criterion = nn.NLLLoss()\n",
    "else:\n",
    "    criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs = 1\n",
    "patience = 5\n",
    "best_loss = np.inf\n",
    "counter = 0\n",
    "batch_size=64\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    for train_data, train_labels, val_data, val_labels in tqdm(train_test_splits):\n",
    "        train_dataset = TensorDataset(train_data, train_labels)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        val_dataset = TensorDataset(val_data, val_labels)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        # Access the batch size from the train_loader\n",
    "        batch_size = train_loader.batch_size\n",
    "        sequence_length = train_loader.dataset.tensors[0].size(1)\n",
    "        # Generate look-ahead masks\n",
    "        train_mask = ScaledMultiHeadAttention.create_look_ahead_mask(batch_size, sequence_length).to(device)\n",
    "        val_mask = ScaledMultiHeadAttention.create_look_ahead_mask(batch_size, sequence_length).to(device)\n",
    "\n",
    "\n",
    "        train_loss = 0.0\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data, src_mask=train_mask).squeeze()\n",
    "            if task_types[0] == 'classification':\n",
    "                labels = labels.long()  # Adjusted here to use the look-ahead mask\n",
    "            loss = criterion(outputs, labels)  # Adjust based on your specific use case\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        total_train_loss += train_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(data, src_mask=val_mask).squeeze()\n",
    "                if task_types[0] == 'classification':\n",
    "                    labels = labels.long() # Adjusted here to use the look-ahead mask\n",
    "                loss = criterion(outputs.squeeze(), labels)  # Adjust based on your specific use case\n",
    "                val_loss += loss.item() * data.size(0)\n",
    "\n",
    "        total_val_loss += val_loss / len(val_loader.dataset)\n",
    "\n",
    "    average_train_loss = total_train_loss / len(train_test_splits)\n",
    "    average_val_loss = total_val_loss / len(train_test_splits)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{n_epochs}, '\n",
    "          f'Average Train Loss: {average_train_loss:.4f}, '\n",
    "          f'Average Validation Loss: {average_val_loss:.4f}')\n",
    "\n",
    "    if average_val_loss < best_loss:\n",
    "        best_loss = average_val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter == patience:\n",
    "        print('Early stopping!')\n",
    "        break\n",
    "\n",
    "best_model_state = torch.load(best_model_path, map_location=device)\n",
    "model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sharpe ratio objective function?\n",
    "# for epoch in range(n_epochs):\n",
    "#     model.train()\n",
    "#     total_train_loss = 0.0\n",
    "#     total_val_loss = 0.0\n",
    "\n",
    "#     for train_data, train_labels, val_data, val_labels in train_test_splits:\n",
    "#         # Make sure your labels are in the correct shape\n",
    "#         train_labels = train_labels.view(-1, 1).to(device)\n",
    "#         val_labels = val_labels.view(-1, 1).to(device)\n",
    "\n",
    "#         # Training section\n",
    "#         train_mask = ScaledMultiHeadAttention.create_look_ahead_mask(train_data.size(1)).to(device)\n",
    "#         train_data = train_data.to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         weights = model(train_data, src_mask=train_mask)\n",
    "#         abs_sum = torch.sum(torch.abs(weights), axis=1, keepdim=True) + 1e-8  # Avoid division by zero\n",
    "#         weights = weights / abs_sum\n",
    "\n",
    "#         rets = torch.sum(weights * train_labels, axis=1)  \n",
    "\n",
    "#         mean_ret = torch.mean(rets)\n",
    "#         std = torch.std(rets) + 1e-8  # Avoid division by zero\n",
    "#         sharpe_ratio = mean_ret / std  \n",
    "\n",
    "#         train_loss = -sharpe_ratio\n",
    "#         train_loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_train_loss += train_loss.item()\n",
    "\n",
    "#         # Validation section\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             val_mask = ScaledMultiHeadAttention.create_look_ahead_mask(val_data.size(1)).to(device)\n",
    "#             val_data = val_data.to(device)\n",
    "\n",
    "#             val_weights = model(val_data, src_mask=val_mask)\n",
    "#             val_abs_sum = torch.sum(torch.abs(val_weights), axis=1, keepdim=True) + 1e-8  # Avoid division by zero\n",
    "#             val_weights = val_weights / val_abs_sum\n",
    "            \n",
    "#             val_rets = torch.sum(val_weights * val_labels, axis=1)  \n",
    "\n",
    "#             val_mean_ret = torch.mean(val_rets)\n",
    "#             val_std = torch.std(val_rets) + 1e-8  # Avoid division by zero\n",
    "#             val_sharpe_ratio = val_mean_ret / val_std\n",
    "\n",
    "#             val_loss = -val_sharpe_ratio\n",
    "#             total_val_loss += val_loss.item()\n",
    "\n",
    "#     avg_train_loss = total_train_loss / len(train_test_splits)\n",
    "#     avg_val_loss = total_val_loss / len(train_test_splits)\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}/{n_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "#     # Inside your training loop\n",
    "#     if avg_val_loss < best_loss:\n",
    "#         best_loss = avg_val_loss\n",
    "#         torch.save(model.state_dict(), 'best_model.pth')\n",
    "#         counter = 0\n",
    "#     else:\n",
    "#         counter += 1\n",
    "\n",
    "#     if counter == patience:\n",
    "#         print('Early stopping!')\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TimeSeriesTransformer(d_model=64, num_heads=8, d_ff=256, num_encoder_layers=2, \n",
    "                               dropout=.1, max_len=240,task_type='classification')\n",
    "model.load_state_dict(torch.load('best_model.pth',map_location=torch.device('cpu')) )\n",
    "model.eval()\n",
    "\n",
    "in_sample_long_portfolios = pd.DataFrame()\n",
    "out_of_sample_long_portfolios = pd.DataFrame()\n",
    "\n",
    "in_sample_short_portfolios = pd.DataFrame()\n",
    "out_of_sample_short_portfolios = pd.DataFrame()\n",
    "\n",
    "k = 10  # Number of top assets to select in portfolios\n",
    "\n",
    "for train_data, train_labels, val_data, val_labels in tqdm(train_test_splits):\n",
    "    # Here, train_data, val_data are your training and validation data respectively\n",
    "    \n",
    "    train_mask = ScaledMultiHeadAttention.create_look_ahead_mask(train_data.size(1))\n",
    "    val_mask = ScaledMultiHeadAttention.create_look_ahead_mask(val_data.size(1))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_predictions = model(train_data.to(device), src_mask=train_mask.to(device))\n",
    "        val_predictions = model(val_data.to(device), src_mask=val_mask.to(device))\n",
    "\n",
    "        train_probs = torch.softmax(train_predictions, dim=1)[:, 1].cpu().numpy()\n",
    "        val_probs = torch.softmax(val_predictions, dim=1)[:, 1].cpu().numpy()\n",
    "\n",
    "    # Assuming you have a dataframe or similar structure to hold the date and TICKER information\n",
    "    train_df['predicted_prob'] = train_probs\n",
    "    val_df['predicted_prob'] = val_probs\n",
    "\n",
    "    # In-Sample Portfolio Construction\n",
    "    for date in train_df['date'].unique():\n",
    "        date_data = train_df[train_df['date'] == date].sort_values(by='predicted_prob', ascending=False)\n",
    "        \n",
    "        long_tickers = date_data.head(k)\n",
    "        short_tickers = date_data.tail(k)\n",
    "        \n",
    "        in_sample_long_portfolios = pd.concat([in_sample_long_portfolios, long_tickers])\n",
    "        in_sample_short_portfolios = pd.concat([in_sample_short_portfolios, short_tickers])\n",
    "\n",
    "    # Out-of-Sample Portfolio Construction\n",
    "    for date in val_df['date'].unique():\n",
    "        date_data = val_df[val_df['date'] == date].sort_values(by='predicted_prob', ascending=False)\n",
    "        \n",
    "        long_tickers = date_data.head(k)\n",
    "        short_tickers = date_data.tail(k)\n",
    "        \n",
    "        out_of_sample_long_portfolios = pd.concat([out_of_sample_long_portfolios, long_tickers])\n",
    "        out_of_sample_short_portfolios = pd.concat([out_of_sample_short_portfolios, short_tickers])\n",
    "\n",
    "# At this point, in_sample_long_portfolios, out_of_sample_long_portfolios, etc. hold your portfolios\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sample_long_portfolios.to_csv('../data/transformer_results/in_sample_long_portfolios.csv')\n",
    "in_sample_short_portfolios.to_csv('../data/transformer_results/in_sample_short_portfolios.csv')\n",
    "out_of_sample_long_portfolios.to_csv('../data/transformer_results/out_of_sample_long_portfolios.csv')\n",
    "out_of_sample_short_portfolios.to_csv('../data/transformer_results/out_of_sample_short_portfolios.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
