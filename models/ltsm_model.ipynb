{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import yfinance as yf\n",
    "import sys\n",
    "sys.path.append('../data_func')\n",
    "from data_helper_functions import create_study_periods,create_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../data/crsp_ff_adjusted.csv')\n",
    "df['RET']=pd.to_numeric(df['RET'],errors='coerce')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.dropna(subset=['RET'],inplace=True)\n",
    "#drop unamed 0 column\n",
    "df.drop(columns=['Unnamed: 0'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>TICKER</th>\n",
       "      <th>RET</th>\n",
       "      <th>Adj_RET_Mkt</th>\n",
       "      <th>Adj_RET_Mkt_SMB</th>\n",
       "      <th>Adj_RET_Mkt_SMB_HML</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990-02-01</td>\n",
       "      <td>SUNW</td>\n",
       "      <td>0.012903</td>\n",
       "      <td>7.292903</td>\n",
       "      <td>8.532903</td>\n",
       "      <td>7.682903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990-02-01</td>\n",
       "      <td>MYG</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>7.294085</td>\n",
       "      <td>8.534085</td>\n",
       "      <td>7.684085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990-02-01</td>\n",
       "      <td>INTC</td>\n",
       "      <td>-0.012658</td>\n",
       "      <td>7.267342</td>\n",
       "      <td>8.507342</td>\n",
       "      <td>7.657342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990-02-01</td>\n",
       "      <td>CB</td>\n",
       "      <td>0.005634</td>\n",
       "      <td>7.285634</td>\n",
       "      <td>8.525634</td>\n",
       "      <td>7.675634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990-02-01</td>\n",
       "      <td>BUD</td>\n",
       "      <td>-0.026490</td>\n",
       "      <td>7.253510</td>\n",
       "      <td>8.493510</td>\n",
       "      <td>7.643510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266862</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>KMI</td>\n",
       "      <td>0.026135</td>\n",
       "      <td>-0.533865</td>\n",
       "      <td>-4.133865</td>\n",
       "      <td>-3.713865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266863</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>ADM</td>\n",
       "      <td>-0.005423</td>\n",
       "      <td>-0.565423</td>\n",
       "      <td>-4.165423</td>\n",
       "      <td>-3.745423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266864</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>HPE</td>\n",
       "      <td>-0.005236</td>\n",
       "      <td>-0.565236</td>\n",
       "      <td>-4.165236</td>\n",
       "      <td>-3.745236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266865</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>DIS</td>\n",
       "      <td>-0.011849</td>\n",
       "      <td>-0.571849</td>\n",
       "      <td>-4.171849</td>\n",
       "      <td>-3.751849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266866</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>-0.551936</td>\n",
       "      <td>-4.151936</td>\n",
       "      <td>-3.731936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3266867 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date TICKER       RET  Adj_RET_Mkt  Adj_RET_Mkt_SMB  \\\n",
       "0       1990-02-01   SUNW  0.012903     7.292903         8.532903   \n",
       "1       1990-02-01    MYG  0.014085     7.294085         8.534085   \n",
       "2       1990-02-01   INTC -0.012658     7.267342         8.507342   \n",
       "3       1990-02-01     CB  0.005634     7.285634         8.525634   \n",
       "4       1990-02-01    BUD -0.026490     7.253510         8.493510   \n",
       "...            ...    ...       ...          ...              ...   \n",
       "3266862 2015-12-31    KMI  0.026135    -0.533865        -4.133865   \n",
       "3266863 2015-12-31    ADM -0.005423    -0.565423        -4.165423   \n",
       "3266864 2015-12-31    HPE -0.005236    -0.565236        -4.165236   \n",
       "3266865 2015-12-31    DIS -0.011849    -0.571849        -4.171849   \n",
       "3266866 2015-12-31   TSLA  0.008064    -0.551936        -4.151936   \n",
       "\n",
       "         Adj_RET_Mkt_SMB_HML  \n",
       "0                   7.682903  \n",
       "1                   7.684085  \n",
       "2                   7.657342  \n",
       "3                   7.675634  \n",
       "4                   7.643510  \n",
       "...                      ...  \n",
       "3266862            -3.713865  \n",
       "3266863            -3.745423  \n",
       "3266864            -3.745236  \n",
       "3266865            -3.751849  \n",
       "3266866            -3.731936  \n",
       "\n",
       "[3266867 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobbarcelona/.pyenv/versions/3.9.16/lib/python3.9/site-packages/pandas/core/frame.py:5034: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n"
     ]
    }
   ],
   "source": [
    "#select returns to use\n",
    "returns='Adj_RET_Mkt'\n",
    "df=df[['date','TICKER',f'{returns}']]\n",
    "if returns!='RET':\n",
    "    #rename returns column\n",
    "    df.rename(columns={f'{returns}':'RET'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 25/29 [00:10<00:01,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached the end of the dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Optional parameter target_type: 'cross_sectional_median(default)','buckets(10 buckets)','raw_returns'.\n",
    "study_periods=create_study_periods(df,n_periods=23,window_size=240,trade_size=250,train_size=750,forward_roll=250,start_date=datetime(1990,1,1),end_date=datetime(2015,12,31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   1 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=6)]: Done   6 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=6)]: Done  17 out of  25 | elapsed:  9.9min remaining:  4.7min\n",
      "[Parallel(n_jobs=6)]: Done  20 out of  25 | elapsed: 12.9min remaining:  3.2min\n",
      "[Parallel(n_jobs=6)]: Done  23 out of  25 | elapsed: 13.1min remaining:  1.1min\n",
      "[Parallel(n_jobs=6)]: Done  25 out of  25 | elapsed: 14.6min finished\n"
     ]
    }
   ],
   "source": [
    "train_test_splits=create_tensors(study_periods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([259482, 240, 1]) torch.Size([259482]) torch.Size([86423, 240, 1]) torch.Size([86423])\n",
      "torch.Size([259523, 240, 1]) torch.Size([259523]) torch.Size([85610, 240, 1]) torch.Size([85610])\n",
      "torch.Size([257790, 240, 1]) torch.Size([257790]) torch.Size([86667, 240, 1]) torch.Size([86667])\n",
      "torch.Size([259178, 240, 1]) torch.Size([259178]) torch.Size([86360, 240, 1]) torch.Size([86360])\n",
      "torch.Size([259115, 240, 1]) torch.Size([259115]) torch.Size([85279, 240, 1]) torch.Size([85279])\n",
      "torch.Size([258784, 240, 1]) torch.Size([258784]) torch.Size([84181, 240, 1]) torch.Size([84181])\n",
      "torch.Size([256298, 240, 1]) torch.Size([256298]) torch.Size([86916, 240, 1]) torch.Size([86916])\n",
      "torch.Size([256854, 240, 1]) torch.Size([256854]) torch.Size([85044, 240, 1]) torch.Size([85044])\n",
      "torch.Size([256619, 240, 1]) torch.Size([256619]) torch.Size([86592, 240, 1]) torch.Size([86592])\n",
      "torch.Size([259030, 240, 1]) torch.Size([259030]) torch.Size([86542, 240, 1]) torch.Size([86542])\n",
      "torch.Size([258656, 240, 1]) torch.Size([258656]) torch.Size([86152, 240, 1]) torch.Size([86152])\n",
      "torch.Size([259764, 240, 1]) torch.Size([259764]) torch.Size([86189, 240, 1]) torch.Size([86189])\n",
      "torch.Size([259361, 240, 1]) torch.Size([259361]) torch.Size([87653, 240, 1]) torch.Size([87653])\n",
      "torch.Size([260472, 240, 1]) torch.Size([260472]) torch.Size([85248, 240, 1]) torch.Size([85248])\n",
      "torch.Size([259568, 240, 1]) torch.Size([259568]) torch.Size([85867, 240, 1]) torch.Size([85867])\n",
      "torch.Size([259246, 240, 1]) torch.Size([259246]) torch.Size([86707, 240, 1]) torch.Size([86707])\n",
      "torch.Size([258300, 240, 1]) torch.Size([258300]) torch.Size([86771, 240, 1]) torch.Size([86771])\n",
      "torch.Size([259823, 240, 1]) torch.Size([259823]) torch.Size([86215, 240, 1]) torch.Size([86215])\n",
      "torch.Size([260171, 240, 1]) torch.Size([260171]) torch.Size([86325, 240, 1]) torch.Size([86325])\n",
      "torch.Size([259789, 240, 1]) torch.Size([259789]) torch.Size([87728, 240, 1]) torch.Size([87728])\n",
      "torch.Size([260746, 240, 1]) torch.Size([260746]) torch.Size([85753, 240, 1]) torch.Size([85753])\n",
      "torch.Size([260284, 240, 1]) torch.Size([260284]) torch.Size([85279, 240, 1]) torch.Size([85279])\n",
      "torch.Size([259238, 240, 1]) torch.Size([259238]) torch.Size([87245, 240, 1]) torch.Size([87245])\n",
      "torch.Size([258755, 240, 1]) torch.Size([258755]) torch.Size([86166, 240, 1]) torch.Size([86166])\n",
      "torch.Size([259168, 240, 1]) torch.Size([259168]) torch.Size([85663, 240, 1]) torch.Size([85663])\n"
     ]
    }
   ],
   "source": [
    "#Optional code to verify tensor shapes\n",
    "# for train_data, train_labels, test_data, test_labels in train_test_splits:\n",
    "#     print(train_data.shape, train_labels.shape, test_data.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now our prediction task is predicting a stocks returns above or below the cross section rolling median of returns. But I want to be able to reject making a prediction, (a -1 choice) when I am not confident in the prediction.\n",
    "We can train a model g(x) that rejects or acepts data to make a prediction and a model f(x) that predicts the returns. We can then combine the two models to make a prediction.\n",
    "\n",
    "If I can somehow make a stochastic transformer, and during testing reject data points that are too far from our self-attention centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Checking if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Define the LSTM Classifier model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, dropout=0.1)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # We only want the last output of the sequence\n",
    "        return out\n",
    "\n",
    "model = LSTMClassifier(input_size=1, hidden_size=25, output_size=2).to(device)  # Move model to GPU if available\n",
    "# Loss depends on target, MAE for returns, Cross Entropy for above/below cross-sectional median. Also have selective loss in utils\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "\n",
    "# Parameters\n",
    "patience = 10\n",
    "n_epochs = 1000\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    for train_data, train_labels, val_data, val_labels in tqdm(train_test_splits):\n",
    "        model.train()\n",
    "\n",
    "        train_data, train_labels = train_data.to(device), train_labels.to(device)  # Move data to GPU if available\n",
    "\n",
    "        # Convert the dataset into DataLoader for batching\n",
    "        train_dataset = TensorDataset(train_data, train_labels)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "        for sequences, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "\n",
    "        val_data, val_labels = val_data.to(device), val_labels.to(device)  # Move data to GPU if available\n",
    "        \n",
    "        # Adding DataLoader for validation data\n",
    "        val_dataset = TensorDataset(val_data, val_labels)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in val_loader:\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter == patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:23<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(best_model_state)\n",
    "model.eval()\n",
    "predictions = []\n",
    "returns_df = df[['date', 'TICKER', 'RET']].copy()\n",
    "# Concatenate training and testing data and labels\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "# This will store DataLoaders for the entire dataset for each period\n",
    "entire_loaders = []\n",
    "\n",
    "for train_loader, test_loader in zip(train_loaders, test_loaders):\n",
    "    # We access the .dataset attribute of the DataLoader to get the underlying dataset\n",
    "    entire_dataset = ConcatDataset([train_loader.dataset, test_loader.dataset])\n",
    "    \n",
    "    # Creating a DataLoader for the entire concatenated dataset\n",
    "    entire_loader = DataLoader(entire_dataset, batch_size=64, shuffle=False)  # We set shuffle to False as we're not training the model here\n",
    "    \n",
    "    entire_loaders.append(entire_loader)\n",
    "\n",
    "# Now, entire_loaders contains DataLoaders for the entire dataset (train + test) for each period\n",
    "\n",
    "# You can now proceed to make predictions for each period like this:\n",
    "all_predictions = []  # This will store predictions for all periods\n",
    "\n",
    "for entire_loader in tqdm(entire_loaders):\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in entire_loader:  \n",
    "            sequences = sequences.unsqueeze(-1)\n",
    "            outputs = model(sequences)\n",
    "            probabilities = torch.softmax(outputs, dim=1)[:, 1].numpy() \n",
    "            predictions.extend(probabilities)\n",
    "            \n",
    "    all_predictions.append(np.array(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# ... (Insert the LSTM model definition and training code here)\n",
    "\n",
    "# Load returns data\n",
    "returns_df = df[['date', 'TICKER', 'RET']].copy()\n",
    "\n",
    "# Create portfolios for both in-sample and out-of-sample periods\n",
    "k = 10\n",
    "unique_dates = predictions_df['date'].unique()\n",
    "portfolios = {'long': [], 'short': []}\n",
    "\n",
    "for date in tqdm(unique_dates):\n",
    "    daily_predictions = predictions_df[predictions_df['date'] == date]\n",
    "    sorted_data = daily_predictions.sort_values(by='probability', ascending=False)\n",
    "    \n",
    "    long_stocks = sorted_data.head(k)['TICKER'].tolist()\n",
    "    short_stocks = sorted_data.tail(k)['TICKER'].tolist()\n",
    "\n",
    "    # Store the tickers for long and short portfolios\n",
    "    portfolios['long'].append(long_stocks)\n",
    "    portfolios['short'].append(short_stocks)\n",
    "\n",
    "# Calculate the daily returns of the portfolios\n",
    "daily_returns = []\n",
    "for date, long_stocks, short_stocks in tqdm(zip(unique_dates, portfolios['long'], portfolios['short'])):\n",
    "    long_returns = returns_df[(returns_df['date'] == date) & (returns_df['TICKER'].isin(long_stocks))]['RET'].mean()\n",
    "    short_returns = returns_df[(returns_df['date'] == date) & (returns_df['TICKER'].isin(short_stocks))]['RET'].mean()\n",
    "\n",
    "    strategy_return = long_returns - short_returns\n",
    "    daily_returns.append(strategy_return)\n",
    "\n",
    "# Calculate performance metrics\n",
    "daily_returns = np.array(daily_returns)\n",
    "mean_daily_return = np.mean(daily_returns)\n",
    "std_daily_return = np.std(daily_returns)\n",
    "annualized_sharpe_ratio = np.sqrt(252) * mean_daily_return / std_daily_return  # Assuming 252 trading days in a year\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Mean Daily Return: {mean_daily_return}\")\n",
    "print(f\"Standard Deviation of Daily Returns: {std_daily_return}\")\n",
    "print(f\"Annualized Sharpe Ratio: {annualized_sharpe_ratio}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
